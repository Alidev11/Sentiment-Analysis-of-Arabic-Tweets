{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleStreamlit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCP/QSqhezTSft6HvVyCQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alidev11/Alidev11/blob/main/SimpleStreamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoRiMiBYoW9x"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "C7u_MI42ofEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3728c80-7fb9-4450-a7b5-8db46c6cc29f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarabic"
      ],
      "metadata": {
        "id": "6hGW3qzBonGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bda2b6-188e-4c93-a816-570fd89f0309"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñã                             | 10 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 20 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 81 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "ibxDz-RPo0e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyngrok\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "!+ streamlit \n",
        "!+ pyngrok\n",
        "!+ nohub/background"
      ],
      "metadata": {
        "id": "r2JvwbzhoqqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "###################### ---------------------------------------------- Import packages --------------------------------------------- ######################\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------- Data Manipulation -------------------------------------------------------------\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import io\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#--------------------------------------------------------------------------- Model -------------------------------------------------------------------\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import *\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.metrics import *\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import svm\n",
        "\n",
        "#-------------------------------------------------------------------------- Visualisation ----------------------------------------------------------------------\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------- Streamlit ---------------------------------------------------------------------------\n",
        "import pyngrok\n",
        "import streamlit as st\n",
        "#-------------------------------------------------------------------------- App Web Code ------------------------------------------------------------------------------------\n",
        "\n",
        "st.set_page_config(page_title= 'Tweet Analyzer',\n",
        "                   page_icon=\"üßä\")\n",
        "st.markdown(\n",
        "            f\"<h1 style='text-align: center; text-transform:uppercase; background: cadetblue; height: 12vh; display: flex; align-items: center; justify-content: center; '>Sentiment Analyzer</h1>\", \n",
        "            unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with st.form(\"my_form\"):\n",
        "    txt = st.text_area('Text to analyze', '''\n",
        "  \n",
        "     ''')\n",
        "\n",
        "    # Every form must have a submit button.\n",
        "    submitted = st.form_submit_button(\"Classify\")\n",
        "    \n",
        "#------------------------------------------Importing Data-----------------------------------------\n",
        "neg_train=pd.read_csv(\"/content/drive/MyDrive/TsvDataset/train_Arabic_tweets_negative_20190413.tsv\", sep='\\t', header=None, encoding='utf-8')\n",
        "pos_train=pd.read_csv(\"/content/drive/MyDrive/TsvDataset/train_Arabic_tweets_positive_20190413.tsv\", sep='\\t', header=None, encoding='utf-8')\n",
        "train_data= pd.concat([pos_train,neg_train],ignore_index=True)\n",
        "train_data.columns=['label','tweet']\n",
        "del pos_train,neg_train\n",
        "\n",
        "neg_test=pd.read_csv(\"/content/drive/MyDrive/TsvDataset/test_Arabic_tweets_negative_20190413.tsv\", sep='\\t', header=None, encoding='utf-8')\n",
        "pos_test=pd.read_csv(\"/content/drive/MyDrive/TsvDataset/train_Arabic_tweets_positive_20190413.tsv\", sep='\\t', header=None, encoding='utf-8')\n",
        "test_data= pd.concat([pos_test,neg_test], ignore_index=True)\n",
        "test_data.columns=['label','tweet']\n",
        "del pos_test,neg_test\n",
        "\n",
        "training_data=train_data.copy()\n",
        "testing_data=test_data.copy()\n",
        "data = pd.concat([training_data,testing_data], ignore_index=True)\n",
        "\n",
        "#----------------------------------- Preprocess -----------------------------------------------\n",
        "punctuations = '''`√∑√óÿõ<>_()*&^%][ŸÄÿå/:\"ÿü.,'{}~¬¶+|!‚Äù‚Ä¶‚Äú‚ÄìŸÄ''' + string.punctuation\n",
        "stop_words = stopwords.words()\n",
        "diac = re.compile(\"\"\"\n",
        "                             Ÿë    | # Shadda\n",
        "                             Ÿé    | # Fatha\n",
        "                             Ÿã    | # Tanwin Fath\n",
        "                             Ÿè    | # Damma\n",
        "                             Ÿå    | # Tanwin Damm\n",
        "                             Ÿê    | # Kasra\n",
        "                             Ÿç    | # Tanwin Kasr\n",
        "                             Ÿí    | # Sukun\n",
        "                             ŸÄ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "\n",
        "import pyarabic.araby as araby\n",
        "def preprocess(text):\n",
        "    translator = str.maketrans('', '', punctuations)\n",
        "    text = text.translate(translator)\n",
        "    \n",
        "    text = araby.strip_tashkeel(text)\n",
        "    text = re.sub(diac, '', text)\n",
        "    text = re.sub(\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n",
        "    text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
        "    text = re.sub(\"ÿ§\", \"ÿ°\", text)\n",
        "    text = re.sub(\"ÿ¶\", \"ÿ°\", text)\n",
        "    text = re.sub(\"ÿ©\", \"Ÿá\", text)\n",
        "    text = re.sub(\"⁄Ø\", \"ŸÉ\", text)\n",
        "    text = re.sub(emoj, '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "training_data.tweet = training_data.tweet.apply(preprocess)\n",
        "testing_data.tweet = testing_data.tweet.apply(preprocess)\n",
        "data = pd.concat([training_data,testing_data], ignore_index=True)\n",
        "\n",
        "#####----------------------------------------------- Data Variables #---------------------------------------------- ####\n",
        "X_train = training_data.tweet.values\n",
        "y_train = training_data.label.values\n",
        "\n",
        "X_test = testing_data.tweet.values\n",
        "y_test = testing_data.label.values\n",
        "pipe = make_pipeline(TfidfVectorizer(),LogisticRegression())\n",
        "model = pipe.fit(X_train,y_train)\n",
        "#####------------------------------------------------- Submit Button #---------------------------------------------- ####\n",
        "\n",
        "if submitted:\n",
        "        txt = preprocess(txt)\n",
        "        txt = [txt]\n",
        "        y_pred = model.predict(txt)\n",
        "        if y_pred[0] == 'neg':\n",
        "          st.write(\"Negative\")\n",
        "        elif y_pred[0] == 'pos':\n",
        "          st.write(\"Positive\")\n",
        "\n",
        "        \n",
        "\n",
        "#####------------------------------------------------- CLASSIFICATION #---------------------------------------------- ####\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_LyBRr1pB4P",
        "outputId": "23ffd920-a7b2-4c63-bc50-a6f31a5e4ac7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!killall ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXOuriNfwG9s",
        "outputId": "08f122a0-3f91-4bf2-d6b6-58e025abcb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok: no process found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 282MoXiHN5HL4bWV1iq4h1H86ul_3GsD3oEgZPCv1jb1SLxk5\n",
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW7SF7G3wHl1",
        "outputId": "a919176a-c3da-4ac6-e38b-740818810a99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-20 13:29:08.771 INFO    pyngrok.ngrok: Opening tunnel named: http-80-77c61221-f630-4cca-b3ea-734cf4e1fc3c\n",
            "2022-05-20 13:29:08.834 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:08+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2022-05-20 13:29:08.840 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:08+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2022-05-20 13:29:08.843 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:08+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2022-05-20 13:29:08.845 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:08+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2022-05-20 13:29:09.007 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2022-05-20 13:29:09.008 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=\"client session established\" obj=csess id=44175c23dfd5\n",
            "2022-05-20 13:29:09.025 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=start pg=/api/tunnels id=0cbba9d13e29c358\n",
            "2022-05-20 13:29:09.028 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=end pg=/api/tunnels id=0cbba9d13e29c358 status=200 dur=486.4¬µs\n",
            "2022-05-20 13:29:09.030 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=start pg=/api/tunnels id=88722c135602a5ae\n",
            "2022-05-20 13:29:09.033 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=end pg=/api/tunnels id=88722c135602a5ae status=200 dur=200.328¬µs\n",
            "2022-05-20 13:29:09.036 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=start pg=/api/tunnels id=a646be9076da3193\n",
            "2022-05-20 13:29:09.127 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-80-77c61221-f630-4cca-b3ea-734cf4e1fc3c (http)\" addr=http://localhost:80 url=http://0768-104-196-49-86.ngrok.io\n",
            "2022-05-20 13:29:09.131 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-80-77c61221-f630-4cca-b3ea-734cf4e1fc3c addr=http://localhost:80 url=https://0768-104-196-49-86.ngrok.io\n",
            "2022-05-20 13:29:09.135 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=end pg=/api/tunnels id=a646be9076da3193 status=201 dur=103.081316ms\n",
            "2022-05-20 13:29:09.137 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=start pg=\"/api/tunnels/http-80-77c61221-f630-4cca-b3ea-734cf4e1fc3c (http)\" id=1babb9be7fa460eb\n",
            "2022-05-20 13:29:09.139 INFO    pyngrok.process.ngrok: t=2022-05-20T13:29:09+0000 lvl=info msg=end pg=\"/api/tunnels/http-80-77c61221-f630-4cca-b3ea-734cf4e1fc3c (http)\" id=1babb9be7fa460eb status=200 dur=188.315¬µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://0768-104-196-49-86.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run --server.port 80 app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "x-9SJOUgwKjW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------ Accuracy --------------------------------\n",
        "        accuracy = model_selection.cross_val_score(model, X_train, y_train, cv=7, scoring='accuracy').mean()\n",
        "        y_pred = model.predict(X_test)\n",
        "        st.write(\"Accuracy \", round(accuracy, 3))\n",
        "\n",
        "        #------------------------------ F1 Score --------------------------------\n",
        "        f1 = make_scorer(f1_score, pos_label=\"pos\")\n",
        "        scoresk=model_selection.cross_val_score(model, X_train, y_train, cv=7, scoring=f1)\n",
        "        f1_val = round(scoresk.mean(), 3)\n",
        "        st.write(f\"F1 score: {f1_val} +/- {scoresk.std():.3f}\")\n",
        "\n",
        "        #------------------------------ Precision --------------------------------\n",
        "        precision = make_scorer(precision_score, pos_label=\"pos\")\n",
        "        scoresk=model_selection.cross_val_score(model, X_train, y_train, cv=7, scoring=precision)\n",
        "        precision_val = round(scoresk.mean(), 3)\n",
        "        st.write(f\"Precision score: {precision_val} +/- {scoresk.std():.3f}\")\n",
        "\n",
        "        #------------------------------ Recall --------------------------------\n",
        "        recall = make_scorer(recall_score, pos_label=\"pos\")\n",
        "        scoresk=model_selection.cross_val_score(model, X_train, y_train, cv=7, scoring=recall)\n",
        "        recall_val = round(scoresk.mean(), 3)\n",
        "        st.write(f\"Recall score: {recall_val} +/- {scoresk.std():.3f}\")\n",
        "\n",
        "        plot_metrics(metrics)\n"
      ],
      "metadata": {
        "id": "YA1gcHg2Lx9_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}